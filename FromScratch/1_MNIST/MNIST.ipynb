{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks form Scratch\n",
    "\n",
    "Neural networks with non-linear activations work like universal function approximators. There is some function that maps your input to the output. For example, images of handwritten digits to class probabilities. The power of neural networks is that we can train them to approximate any function given enough data and compute time.\n",
    "\n",
    "At first the network is naive, it doesn't know the function mapping the inputs to the outputs. We train the network by showing it examples of real data, then adjusting the network parameters such that it approximates this function.\n",
    "\n",
    "To find these parameters, we need to know how poorly the network is predicting the real outputs. For this we calculate a **loss function** (also called the cost), a measure of our prediction error. \n",
    "\n",
    "By minimizing this loss with respect to the network parameters, we can find configurations where the loss is at a minimum and the network is able to predict the correct labels with high accuracy. We find this minimum using a process called **gradient descent**. \n",
    "\n",
    "The gradient is the slope of the loss function and points in the direction of fastest change. To get to the minimum in the least amount of time, we then want to follow the gradient (downwards)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Training multilayer networks is done through **backpropagation** which is really just an application of the chain rule from calculus. \n",
    "\n",
    "In the forward pass through the network, our data and operations go from bottom to top here. We pass the input $x$ through a linear transformation $L_1$ with weights $W_1$ and biases $b_1$. The output then goes through the sigmoid operation $S$ and another linear transformation $L_2$. Finally we calculate the loss $\\ell$. We use the loss as a measure of how bad the network's predictions are. The goal then is to adjust the weights and biases to minimize the loss.\n",
    "\n",
    "To train the weights with gradient descent, we propagate the gradient of the loss backwards through the network. Each operation has some gradient between the inputs and outputs. As we send the gradients backwards, we multiply the incoming gradient with the gradient for the operation. Mathematically, this is really just calculating the gradient of the loss with respect to the weights using the chain rule.\n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial S}{\\partial L_1} \\frac{\\partial L_2}{\\partial S} \\frac{\\partial \\ell}{\\partial L_2}\n",
    "$$\n",
    "\n",
    "\n",
    "The weights are updates using this gradient, scaled by a learning rate $\\alpha$. \n",
    "\n",
    "$$\n",
    "\\large W^\\prime_1 = W_1 - \\alpha \\frac{\\partial \\ell}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "The learning rate $\\alpha$ is set such that the weight update steps are small enough that the iterative method settles in a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses in PyTorch\n",
    "\n",
    "Through the `nn` module, PyTorch provides losses such as the Negative Log-likelihood loss (`nn.NLLLoss()`). Usually the loss is assigned to a variable named `criterion`. \n",
    "\n",
    "With a classification problem such as MNIST, the softmax function is used to predict class **probabilities**.\n",
    "\n",
    "This means we need to pass in the raw output of our network into the loss, not the output of the softmax function. This raw output is usually called the *logits* or *scores*. We use the logits because softmax gives you probabilities which will often be very close to zero or one but floating-point numbers can't accurately represent values near zero or one ([read more here](https://docs.python.org/3/tutorial/floatingpoint.html)). \n",
    "\n",
    "> **Note:** It's usually best to avoid doing calculations with probabilities, typically we use log-probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# import torch.nn.functional as F # Used when defining a Class\n",
    "from torchvision import datasets, transforms\n",
    "from torch import optim\n",
    "\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "                              ])\n",
    "\n",
    "# Download and load the training data\n",
    "train_set = datasets.MNIST('Data/', download=True, train=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to build the model with a log-softmax output using `nn.LogSoftmax` or `F.log_softmax` ([documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.LogSoftmax)). \n",
    "\n",
    "The actual probabilites can be obtained by taking the exponential `torch.exp(output)`:\n",
    "$$P(x) = \\exp^{(output)} $$\n",
    "\n",
    "With a log-softmax output, you want to use the negative log likelihood loss, `nn.NLLLoss` ([documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "Use the loss to perform backpropagation in order to calibrate the weights.\n",
    "> Torch provides a module, `autograd`, for automatically calculating the gradients of tensors. \n",
    "\n",
    "`Autograd` works by keeping track of operations performed on tensors, then going backwards through those operations, calculating gradients along the way. \n",
    "\n",
    "You can turn on or off gradients altogether with `torch.set_grad_enabled(True|False)`, using a content manager `with`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network!\n",
    "\n",
    "There's one last piece we need to start training, an optimizer that we'll use to update the weights with the gradients. We get these from PyTorch's [`optim` package](https://pytorch.org/docs/stable/optim.html). For example we can use stochastic gradient descent with `optim.SGD`. You can see how to define an optimizer below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summing up:\n",
    "1. Run data through the network (Feed forward) to obtain an output (prediction/classification) \n",
    "2. Calculate the loss based on the output and known value\n",
    "3. Backpropagate the loss (`loss.backward()`), calculating the gradients for each step. Once we have the gradients, we can make a gradient descent step for each weight. \n",
    "4. Using the Optimizer, perform a gradient descent step in the direction that minimizes the error, updating the weights.\n",
    "\n",
    "> **Note**: while training, the model will repeat the previous steps multiple times. It is critical to zero the gradients (`optimizer.zero_grad()`) on each training pass, otherwise it will retain gradients from previous steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Neural Network\n",
    "\n",
    "Now we will train the network for all the images.\n",
    "\n",
    "**Epoch**: one pass through the entire dataset.\n",
    "**Batch**: while looping through the `train_loader`, specify how many images will be used at a time.\n",
    "\n",
    "For each batch, we perform a training pass: \n",
    "+ calculate the loss\n",
    "+ do a backwards pass\n",
    "+ update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.8750 | Time: 8 sec\n",
      "Training loss: 0.8423 | Time: 16 sec\n",
      "Training loss: 0.5165 | Time: 25 sec\n",
      "Training loss: 0.4265 | Time: 33 sec\n",
      "Training loss: 0.3848 | Time: 41 sec\n",
      "Training loss: 0.3598 | Time: 50 sec\n",
      "Training loss: 0.3423 | Time: 58 sec\n",
      "Training loss: 0.3285 | Time: 66 sec\n",
      "Training loss: 0.3171 | Time: 75 sec\n",
      "Training loss: 0.3075 | Time: 83 sec\n"
     ]
    }
   ],
   "source": [
    "input_vector = 28*28 # size of 28 by 28 pixel image, flattened to a single vector\n",
    "output_classes = 10 #total classes to be predicted\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_vector, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, output_classes),\n",
    "                      nn.LogSoftmax(dim=1)) # Calculate along Columns, so each row sums 1\n",
    "\n",
    "criterion = nn.NLLLoss() # Negative Log-likelihood loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = len(train_loader)\n",
    "\n",
    "start = time()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # Reset the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        elapsed = time()\n",
    "    \n",
    "    print(f\"Training loss: {running_loss/batch_size :.4f} | Time: {elapsed-start :.0f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the network trained, we can check out it's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_classify(image, probability):\n",
    "    # Preparing data to be used\n",
    "    probability = probability.data.numpy().squeeze() # remove batch information\n",
    "    image = img.resize(1,28,28) # from flatten to matrix structure\n",
    "    image = image.numpy().squeeze() # remove batch information\n",
    "    \n",
    "    # Plotting actual image and Classification Probabilities\n",
    "    fig, axes = plt.subplots(figsize=(6,9), ncols=2)\n",
    "\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].axis('off')\n",
    "    axes[0].set_title('Actual Image')\n",
    "\n",
    "    axes[1].barh(range(10), probability)\n",
    "    axes[1].set_aspect(0.1)\n",
    "    axes[1].set_yticks(range(10))\n",
    "    axes[1].set_yticklabels(range(10))\n",
    "    axes[1].set_xlim(0,1)\n",
    "    axes[1].set_title('Class Probability')\n",
    "    plt.tight_layout();\n",
    "\n",
    "    # Print Classification\n",
    "    highest_class = probability.argmax() # returns index with highest value\n",
    "    print(f'Predicted Digit: {highest_class} | Likelihood: {probability[highest_class]:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Digit: 9 | Likelihood: 86.60%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f0630c5630>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for images, labels in train_loader:\n",
    "\n",
    "    img = images[0].view(1, 784) # Flatten image into vector\n",
    "    \n",
    "    # Turn off gradients to speed up this part\n",
    "    with torch.no_grad():\n",
    "        logit = model.forward(img)\n",
    "\n",
    "    # Output of the network are logits, need to take softmax for probabilities\n",
    "    probability = torch.exp(logit)\n",
    "    \n",
    "view_classify(img, probability)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
